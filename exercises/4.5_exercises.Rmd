---
title: "4.5_exercises"
author: "Kurth"
date: "2025-02-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 4.5 Exercises

```{r setup, echo=FALSE}
library(stringr)
library(visdat)
library(tidyr)
library(readr)
library(stringr)
library(purrr)
library(readr)
library(dplyr)
library(lubridate)
library(ggplot2)
```

## Star wars 
```{r starwars}
starwars <- dplyr::starwars
starwars
```

### How many pale characters come from the planets Ryloth and Naboo?
```{r starwars1}
dplyr::starwars |>
  filter(str_detect(skin_color, "pale"),
         homeworld %in% c("Ryloth", "Naboo")) |>
  select(name, skin_color, homeworld) |>
  nrow()
```

### Who is the oldest among the tallest thirty characters?
```{r starwars2}
dplyr::starwars |>
  filter(!is.na(height), !is.na(birth_year)) |>
  arrange(desc(height)) |>
  slice_head(n = 30) |>
  arrange(birth_year) |>
  select(name, height, birth_year) |>
  head(1)
```

### What is the name of the smallest character and their starship in “Return of the Jedi”
```{r starwars3}
dplyr::starwars |>
  filter(!is.na(height)) |>
  unnest(cols = films) |>
  filter(str_detect(films, "Return of the Jedi")) |>
  nest(cols = films) |>
  arrange(height) |>
  select(name, height, starships) |>
  slice_head(n = 1) |>
  unnest(cols = starships, keep_empty = TRUE)
```

## Aggregating
### Reuse the code in the tutorial to read, reduce, and aggregate the half_hourly_fluxes dataset to the daily scale, calculating the following metrics across half-hourly VPD_F values within each day: mean, 25% quantile, and 75% quantile.

```{r aggregating1}
half_hourly_fluxes <- read_csv("./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv")
daily_fluxes <- half_hourly_fluxes |>
  mutate(across(starts_with("TIMESTAMP_"), ymd_hm)) |>
  mutate(date = as_date(TIMESTAMP_START)) |> # converts time object to a date object
  group_by(date) |>
  summarise(
    VPD_F_mean = mean(VPD_F, na.rm = TRUE),
    # Mean
    VPD_F_q25  = quantile(VPD_F, 0.25, na.rm = TRUE),
    # 25% quantile
    VPD_F_q75  = quantile(VPD_F, 0.75, na.rm = TRUE) # 75% quantile
  )

daily_fluxes
```
### Retain only the daily data for which the daily mean VPD is among the upper or the lower 10% quantiles.
```{r aggregating2}
half_hourly_fluxes <- read_csv("./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv")
daily_fluxes <- half_hourly_fluxes |>
  mutate(across(starts_with("TIMESTAMP_"), ymd_hm)) |>
  mutate(date = as_date(TIMESTAMP_START)) |> # converts time object to a date object
  group_by(date) |>
  summarise(VPD_F_mean = mean(VPD_F, na.rm = TRUE))
lower_10 <- quantile(daily_fluxes$VPD_F_mean, 0.1, na.rm = TRUE)
upper_10 <- quantile(daily_fluxes$VPD_F_mean, 0.9, na.rm = TRUE)
filtered_daily_fluxes <- daily_fluxes |>
  filter(VPD_F_mean <= lower_10 | VPD_F_mean >= upper_10)
daily_fluxes
```
### Calculate the mean of the 25% and the mean of the 75% quantiles of half-hourly VPD within the upper and lower 10% quantiles of mean daily VPD.
```{r aggregating3}
half_hourly_fluxes <- read_csv("./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv")

# Calculate mean daily VPD
daily_fluxes <- half_hourly_fluxes |>
  mutate(date = as_date(TIMESTAMP_START)) |> # converts time object to a date object
  group_by(date) |>
  summarise(VPD_F_mean = mean(VPD_F, na.rm = TRUE))

# Calculate upper and lower 10% of mean daily VPD
lower_10 <- quantile(daily_fluxes$VPD_F_mean, 0.1, na.rm = TRUE)
upper_10 <- quantile(daily_fluxes$VPD_F_mean, 0.9, na.rm = TRUE)

# Filter for days in upper and lower 10% quantiles of mean daily VPD
filtered_VPD_F_mean <- daily_fluxes |>
  filter(VPD_F_mean <= lower_10 | VPD_F_mean >= upper_10)

# Join back with the half-hourly data set to get data for the days withing the lower and upper 10% quantiles of mean daily VPD
filtered_half_hourly_fluxes <- half_hourly_fluxes |>
  mutate(date = as_date(TIMESTAMP_START)) |>  # Convert to daily format
  semi_join(filtered_VPD_F_mean, by = "date")

# Calculate the 25% and 75% quarantines of half-hourly VPD within these days
VPD_q25_mean <- quantile(filtered_half_hourly_fluxes$VPD_F, 0.25, na.rm = TRUE)
VPD_q75_mean <- quantile(filtered_half_hourly_fluxes$VPD_F, 0.75, na.rm = TRUE)

cat("Mean of 25% quantile of half-hourly VPD:", VPD_q25_mean, "\n")
cat("Mean of 75% quantile of half-hourly VPD:", VPD_q75_mean, "\n")
```

## Patterns in data quality
## (i) actually measured data,
## (ii) good quality gap-filled data, 
## (iii) medium quality data, 
## and (iv) poor quality data within each hour-of-day (24 hours per day).
```{r patterns}
# Read dataset and clean missing values (-9999 → NA)
half_hourly_fluxes <- read_csv("./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv") |>
  mutate(across(where(is.numeric), ~ na_if(., -9999)))  # Convert -9999 to NA

# Compute NEE Data Quality Proportions across Hours
NEE_hourly_data_quality <- half_hourly_fluxes |>
  mutate(hour = hour(ymd_hm(TIMESTAMP_START))) |> # Extract hourly
  group_by(hour) |>
  summarize(
    total = n(),
    # Total records per hour
    actually_measured = sum(NEE_VUT_REF_QC == 0, na.rm = TRUE),
    good_quality = sum(NEE_VUT_REF_QC == 1, na.rm = TRUE),
    medium_quality = sum(NEE_VUT_REF_QC == 2, na.rm = TRUE),
    poor_quality = sum(NEE_VUT_REF_QC == 3, na.rm = TRUE)
  ) |>
  mutate(
    prop_measured = actually_measured / total,
    prop_good = good_quality / total,
    prop_medium = medium_quality / total,
    prop_poor = poor_quality / total
  )
# Print the NEE data quality proportions per hour
print(NEE_hourly_data_quality)
NEE_hourly_data_quality
ggplot(NEE_hourly_data_quality, aes(x = hour)) +
  geom_line(aes(y = prop_measured, color = "Measured"), size = 1) +
  geom_line(aes(y = prop_good, color = "Good Gap-filled"), size = 1) +
  geom_line(aes(y = prop_medium, color = "Medium Gap-filled"), size = 1) +
  geom_line(aes(y = prop_poor, color = "Poor Gap-filled"), size = 1) +
  labs(title = "Proportion of NEE Data Quality by Hour", x = "Hour of Day", y = "Proportion") +
  scale_color_manual(
    values = c(
      "Measured" = "blue",
      "Good Gap-filled" = "green",
      "Medium Gap-filled" = "orange",
      "Poor Gap-filled" = "red"
    )
  ) +
  theme_minimal()


# 2. Aggregate Half-Hourly GPP Data to Daily Means
# a) Using the full data set (all GPP_NT_VUT_REF values)
GPP_daily <- half_hourly_fluxes |>
  mutate(date = as_date(TIMESTAMP_START)) |> # Convert time stamp to date
  group_by(date) |>
  summarize(GPP_mean = mean(GPP_NT_VUT_REF, na.rm = TRUE)) # Compute daily mean


# b) Using only actually measured data (QC == 0)
GPP_daily_measured <- half_hourly_fluxes |>
  filter(NEE_VUT_REF_QC == 0) |> # Filter measured data
  mutate(date = as_date(TIMESTAMP_START)) |>
  group_by(date) |>
  summarize(GPP_mean_measured = mean(GPP_NT_VUT_REF, na.rm = TRUE))

# 3. Compute Overall Mean GPP for Both Versions
overall_GPP <- mean(GPP_daily$GPP_mean, na.rm = TRUE)
overall_GPP_measured <- mean(GPP_daily_measured$GPP_mean_measured, na.rm = TRUE)

cat("Overall Mean GPP (Full Dataset):", overall_GPP, "\n")
cat("Overall Mean GPP (Only Measured Data):",
    overall_GPP_measured,
    "\n")

ggplot() +
  geom_density(data = GPP_daily,
               aes(x = GPP_mean, fill = "Full Dataset"),
               alpha = 0.5) +
  geom_density(data = GPP_daily_measured,
               aes(x = GPP_mean_measured, fill = "Measured Only"),
               alpha = 0.5) +
  labs(title = "Comparison of Daily GPP Distributions", x = "GPP (gC m^-2 d^-1)", y = "Density") +
  scale_fill_manual(values = c(
    "Full Dataset" = "blue",
    "Measured Only" = "red"
  )) +
  theme_minimal()
```
